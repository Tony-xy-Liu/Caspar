{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import requests\n",
    "from local.caching import load, save, DictCache\n",
    "from local.web import ncbi_search, chain_get\n",
    "from local.constants import WORKSPACE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching result 1246 of 1246"
     ]
    }
   ],
   "source": [
    "ERR, RESPONSES = ncbi_search(\"cyanobacteria genomic models\", \"pubmed\", \"efetch\", search_params=[(\"retmax\", \"10000\")], response_params=[(\"rettype\", \"abstract\")])\n",
    "# print(json.dumps(d[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_kk = set()\n",
    "def get_summary(data: dict):\n",
    "    if \"PubmedBookArticle\" in data[\"PubmedArticleSet\"]:\n",
    "        print(\"skipping book\")\n",
    "        return None\n",
    "\n",
    "    x = chain_get(data, \"PubmedArticleSet, PubmedArticle, MedlineCitation, Article, ArticleTitle\")\n",
    "    assert len(x) == 1, x\n",
    "    title = x[0]\n",
    "    if isinstance(title, dict):\n",
    "        title = \"\\n\".join([s if isinstance(s, str) else \" \".join(s) for s in title.values()])\n",
    "\n",
    "    x = chain_get(data, \"PubmedArticleSet, PubmedArticle, MedlineCitation, Article, Abstract, AbstractText\")\n",
    "    abstract = []\n",
    "    if x is not None:\n",
    "        for d in x:\n",
    "            if isinstance(d, str):\n",
    "                abstract.append(d)\n",
    "            else:\n",
    "                for k, v in d.items():\n",
    "                    if k != \"#text\": \n",
    "                        _kk.add(k)\n",
    "                        continue\n",
    "                    try:\n",
    "                        if isinstance(v, str): abstract.append(v)\n",
    "                        elif isinstance(v, list): abstract.append(\" \".join(set(v)))\n",
    "                        elif isinstance(v, dict): abstract.append(\" \".join(v.values()))\n",
    "                    except:\n",
    "                        abstract.append(json.dumps(v))\n",
    "        \n",
    "    x = chain_get(data, \"PubmedArticleSet, PubmedArticle, PubmedData, ArticleIdList, ArticleId\")\n",
    "    if x is None:\n",
    "        doi = \"\"\n",
    "    else:\n",
    "        doi = None\n",
    "        for entry in x:\n",
    "            if \"doi\" in entry.values():\n",
    "                doi = entry[\"#text\"]\n",
    "        if doi is None:\n",
    "            doi = \"\"\n",
    "\n",
    "    return doi, title, \"\\n\".join(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1245"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./cache/abstracts.txt\", \"w\") as f:\n",
    "    all_entries = []\n",
    "    for e in RESPONSES:\n",
    "        toks = get_summary(e)\n",
    "        if toks is None: continue\n",
    "        for x in toks:\n",
    "            assert isinstance(x, str), x\n",
    "        doi, t, a = toks\n",
    "\n",
    "        entry = \"\".join([f\"{x}\" for x in [\n",
    "            f\"{t}\",\n",
    "            f\"{a}\",\n",
    "        ]])\n",
    "        f.write(entry+\"\\n\\n\")\n",
    "        all_entries.append((doi, entry))\n",
    "\n",
    "len(all_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DictCache(\"ada_embeddings\") as cache:\n",
    "    for k, v in cache.items():\n",
    "        # print(v)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245 of 1245"
     ]
    }
   ],
   "source": [
    "with open(WORKSPACE_ROOT.joinpath(\"secrets/openai_key\")) as s:\n",
    "    OPENAI_KEY = s.readline()\n",
    "def get_embedding(entry: str):\n",
    "    MAX_L = 8191\n",
    "    if len(entry) > MAX_L:\n",
    "        e = entry[:MAX_L]\n",
    "        print(f\"truncated to {MAX_L} from {len(entry)}\")\n",
    "    else:\n",
    "        e = entry\n",
    "\n",
    "    with DictCache(\"ada_embeddings\") as cache:\n",
    "        if e in cache:\n",
    "            return cache[e]\n",
    "        else:\n",
    "            r = requests.post(\n",
    "                url=\"https://api.openai.com/v1/embeddings\",\n",
    "                headers={\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                    \"Authorization\": f\"Bearer {OPENAI_KEY}\",\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": \"text-embedding-ada-002\",\n",
    "                    \"input\": e,\n",
    "                }\n",
    "            )\n",
    "            data = r.json()\n",
    "            if r.status_code == 200:\n",
    "                cache[e] = data\n",
    "            return data\n",
    "        \n",
    "embeddings = []\n",
    "for i, (doi, text) in enumerate(all_entries):\n",
    "    print(f\"\\r{i+1} of {len(all_entries)}\", end=\"\")\n",
    "    d = get_embedding(text)\n",
    "    embeddings.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping book\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mapping = []\n",
    "i = 0\n",
    "for e in RESPONSES:\n",
    "    toks = get_summary(e)\n",
    "    if toks is None: continue\n",
    "    doi, t, a = toks\n",
    "    emb = embeddings[i]['data'][0]['embedding']\n",
    "    mapping.append((doi, t, a, np.array(emb, dtype=np.float64)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressing & caching data to [{WORKSPACE}/main/scratch/cache/1k_cy_mapping.pkl.gz]\n"
     ]
    }
   ],
   "source": [
    "save(f\"1k_cy_mapping\", mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
